{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ruize Li**\n",
    "\n",
    "Spring 2020\n",
    "\n",
    "CS 251: Data Analysis and Visualization\n",
    "\n",
    "Project 6: Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Naive Bayes Classifier\n",
    "\n",
    "After finishing your email preprocessing pipeline, implement the one other supervised learning algorithm we we will use to classify email, **Naive Bayes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a) Implement Naive Bayes\n",
    "\n",
    "In `naive_bayes.py`, implement the following methods:\n",
    "- Constructor\n",
    "- `train(data, y)`: Train the Naive Bayes classifier so that it records the \"statistics\" of the training set: class priors (i.e. how likely an email is in the training set to be spam or ham?) and the class likelihoods (the probability of a word appearing in each class â€” spam or ham).\n",
    "- `predict(data)`: Combine the class likelihoods and priors to compute the posterior distribution. The predicted class for a test sample is the class that yields the highest posterior probability.\n",
    "- `accuracy(y, y_pred)`: The usual definition :)\n",
    "\n",
    "\n",
    "#### Bayes rule ingredients: Priors and likelihood (`train`)\n",
    "\n",
    "To compute class predictions (probability that a test example belong to either spam or ham classes), we need to evaluate **Bayes Rule**. This means computing the priors and likelihoods based on the training data.\n",
    "\n",
    "**Prior:** $$P_c = \\frac{N_c}{N}$$ where $P_c$ is the prior for class $c$ (spam or ham), $N_c$ is the number of training samples that belong to class $c$ and $N$ is the total number of training samples.\n",
    "\n",
    "**Likelihood:** $$L_{c,w} = \\frac{N_{c,w} + 1}{N_{c} + M}$$ where\n",
    "- $L_{c,w}$ is the likelihood that word $w$ belongs to class $c$ (*i.e. what we are solving for*)\n",
    "- $N_{c,w}$ is the total count of **word $w$** in emails that are only in class $c$ (*either spam or ham*)\n",
    "- $N_{c}$ is the total number of **all words** that appear in emails of the class $c$ (*total number of words in all spam emails or total number of words in all ham emails*)\n",
    "- $M$ is the number of features (*number of top words*).\n",
    "\n",
    "#### Bayes rule ingredients: Posterior (`predict`)\n",
    "\n",
    "To make predictions, we now combine the prior and likelihood to get the posterior:\n",
    "\n",
    "**Posterior:** $$\\text{Post}_{i, c} = Log(P_c) + \\sum_{j \\in J_i}Log(L_{c,j})$$ where\n",
    "- $\\text{Post}_c$ is the posterior for class $c$ for test sample $i$(*i.e. evidence that email $i$ is spam or ham*). What we are solving for.\n",
    "- $Log(P_c)$ is the logarithm of the prior for class $c$ $P_c$.\n",
    "- $j \\in J_i$ (under the sum) indexes the set of words in the current test sample that have nonzero counts (*i.e. which words show up in the current test set email $i$? $j$ is the index of each of these words.*)\n",
    "- $\\sum_{j \\in J_i}Log(L_{c,j})$: we sum over the log-likelihoods ONLY PERTAINING TO CLASS $c$ at word word indices that appear in the current test email $i$ (i.e. indices at which the counts are > 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69315 0.69315 0.     ]\n",
      " [1.60944 2.19722 1.38629]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([2,2,1,5,9, 4]).reshape((2,3))\n",
    "print(np.log(a))\n",
    "# print(np.where(a==0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naive_bayes_multinomial import NaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your class priors are: [0.24 0.26 0.25 0.25]\n",
      "and should be          [0.24 0.26 0.25 0.25].\n",
      "Your class likelihoods shape is (4, 6) and should be (4, 6).\n",
      "Your likelihoods are:\n",
      "[[0.15116 0.18497 0.17571 0.1463  0.16813 0.17374]\n",
      " [0.16695 0.17437 0.15742 0.16887 0.15677 0.17562]\n",
      " [0.14116 0.1562  0.19651 0.17046 0.17951 0.15617]\n",
      " [0.18677 0.18231 0.15884 0.12265 0.16755 0.18187]]\n",
      "and should be\n",
      "[[0.15116 0.18497 0.17571 0.1463  0.16813 0.17374]\n",
      " [0.16695 0.17437 0.15742 0.16887 0.15677 0.17562]\n",
      " [0.14116 0.1562  0.19651 0.17046 0.17951 0.15617]\n",
      " [0.18677 0.18231 0.15884 0.12265 0.16755 0.18187]]\n"
     ]
    }
   ],
   "source": [
    "num_test_classes = 4\n",
    "np.random.seed(0)\n",
    "data_test = np.random.random(size=(100, 6))\n",
    "y_test = np.random.randint(low=0, high=num_test_classes, size=(100,))\n",
    "\n",
    "nbc = NaiveBayes(num_classes=num_test_classes)\n",
    "nbc.train(data_test, y_test)\n",
    "\n",
    "print(f'Your class priors are: {nbc.class_priors}\\nand should be          [0.24 0.26 0.25 0.25].')\n",
    "print(f'Your class likelihoods shape is {nbc.class_likelihoods.shape} and should be (4, 6).')\n",
    "print(f'Your likelihoods are:\\n{nbc.class_likelihoods}')\n",
    "\n",
    "\n",
    "test_likelihoods = np.array([[0.15116, 0.18497, 0.17571, 0.1463 , 0.16813, 0.17374],\n",
    "       [0.16695, 0.17437, 0.15742, 0.16887, 0.15677, 0.17562],\n",
    "       [0.14116, 0.1562 , 0.19651, 0.17046, 0.17951, 0.15617],\n",
    "       [0.18677, 0.18231, 0.15884, 0.12265, 0.16755, 0.18187]])\n",
    "print(f'and should be\\n{test_likelihoods}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your predicted classes are [2 2 2 2] and should be [2 2 2 2].\n"
     ]
    }
   ],
   "source": [
    "num_test_classes = 4\n",
    "np.random.seed(0)\n",
    "data_train = np.random.random(size=(100, 10))\n",
    "data_test = np.random.random(size=(4, 10))\n",
    "y_test = np.random.randint(low=0, high=num_test_classes, size=(100,))\n",
    "\n",
    "nbc = NaiveBayes(num_classes=num_test_classes)\n",
    "nbc.train(data_train, y_test)\n",
    "test_y_pred = nbc.predict(data_test)\n",
    "\n",
    "print(f'Your predicted classes are {test_y_pred} and should be [2 2 2 2].')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c) Spam filtering\n",
    "\n",
    "Let's start classifying spam email using the Naive Bayes classifier.\n",
    "\n",
    "- Use `np.load` to load in the train/test split that you created last week.\n",
    "- Use your Naive Bayes classifier on the Enron email dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9:** What accuracy do you get on the test set with Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got `89%` accuracy on the test set with Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email_preprocessor as epp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your training and test data into numpy ndarrays using np.load()\n",
    "# (the files you created at the end of the previous notebook)\n",
    "\n",
    "\n",
    "x_train = np.load('data/email_train_x.npy')\n",
    "y_train = np.load('data/email_train_y.npy')\n",
    "inds_train = np.load('data/email_train_inds.npy')\n",
    "x_test = np.load('data/email_test_x.npy')\n",
    "y_test = np.load('data/email_test_y.npy')\n",
    "inds_test = np.load('data/email_test_inds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct your classifier\n",
    "nb = NaiveBayes(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is :  0.8902077151335311\n"
     ]
    }
   ],
   "source": [
    "# Train and test your classifier\n",
    "nb.train(x_train, y_train)\n",
    "y_pred = nb.predict(x_test)\n",
    "acc = nb.accuracy(y_test, y_pred)\n",
    "print(\"accuracy is : \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d) Confusion matrix\n",
    "\n",
    "To get a better sense of the errors that the Naive Bayes classifer makes, you will create a confusion matrix. \n",
    "\n",
    "- Implement `confusion_matrix` in `naive_bayes.py`.\n",
    "- Print out a confusion matrix of the spam classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2756.  566.]\n",
      " [ 173. 3245.]]\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test.astype('int')\n",
    "y_pred = y_pred.astype('int')\n",
    "print(nb.confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10:** Interpret the confusion matrix, using the convention that positive detection means spam (*e.g. a false positive means classifying a ham email as spam*). What types of errors are made more frequently by the classifier? What does this mean (*i.e. X (spam/ham) is more likely to be classified than Y (spam/ham) than the other way around*)?\n",
    "\n",
    "**Reminder: Look back at your preprocessing code: which class indices correspond to spam/ham?**\n",
    "\n",
    "**Answer 10:** The confusion matrix tells us that `173` spam emails are falsely classfied as `ham` and 566 `ham` emails are misclassified as `spam`. In terms of binary performance metrics, it makes `False Positive` more frequently. It means that `ham` emails are more likely to be classified as `spam`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e) Investigate the misclassification errors\n",
    "\n",
    "Numbers are nice, but they may not the best for developing your intuition. Sometimes, you want to see what an misclassification *actually* looks like to build your understanding as you look to improve your algorithm. Here, you will take a false positive and a false negative misclassification and retrieve the actual text of the email so see which emails produced the error.\n",
    "\n",
    "- Determine the index of the **FIRST** false positive and false negative misclassification â€” i.e. 2 indices in total. Remember to use your inds array to figure out the index of the emails BEFORE shuffling happened.\n",
    "- **Section B:** Implement the function `retrieve_emails` in `email_preprocessor.py` to return the string of the raw email at the error indices. (**Sections A/C** have been supplied with this function on Classroom.)\n",
    "- Call your function to print out the two emails that produced misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11:** What do you think it is about each email that resulted in it being misclassified?\n",
    "\n",
    "**Answer 11:** For the first scenario, the email has lots of numbers and words related to money, which causes the classfier to detect it as `spam`. For the second email, the message is extremely short and the classfier simply does not have enough information, and the email is easily classified incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3814\n",
      "28708\n"
     ]
    }
   ],
   "source": [
    "# Determine the indices of the 1st FP and FN.\n",
    "# Note: spam = 0, ham = 1\n",
    "inds = np.where(y_pred != y_test)[0]\n",
    "# print(inds.shape)\n",
    "# print(y_test[:10])\n",
    "\n",
    "fp = inds_test[inds[np.in1d(inds, np.where(y_pred == 0)[0])][0]]\n",
    "fn = inds_test[inds[np.in1d(inds, np.where(y_pred == 1)[0])][0]]\n",
    "print(fp)\n",
    "print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3814 28708]\n",
      "\n",
      "The 1st email that is a false positive (classified as spam, but really not) is:\n",
      "------------------------------------------------------------------------------------------\n",
      "Subject: hearing info\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "The 1st email that is a false negative (classified as ham, but really spam) is:\n",
      "------------------------------------------------------------------------------------------\n",
      "Subject: conoco / tw at monument - - - level \" a \" + / - 30 % cost estimate\n",
      "design volume = 60 mmcf / d at 700 psig\n",
      "the following is a summary of facility requirements and the associated cots to tie conoco to tw - 10 \" suction side of the monument compressor station .\n",
      "facilities a - release costs ( $ )\n",
      "10 x 8 \" tee & side valve 30 , 000\n",
      "efm , das , rtu , chromatograh 130 , 000\n",
      "and h 2 o analyzer and buildings .\n",
      "8 \" - ultrasonic measurements 40 , 000\n",
      "flow control valve with pressure override 135 , 000\n",
      "and isolation valve\n",
      "overhead 15 % 50 , 250\n",
      "contingency 10 % 33 , 500\n",
      "total $ 418 , 750\n",
      "if you have any questions please call .\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Use retrieve_emails() to display the first FP and FN.\n",
    "inds = np.array([fp, fn])\n",
    "print(inds)\n",
    "emails = epp.retrieve_emails(inds)\n",
    "\n",
    "print()\n",
    "print('The 1st email that is a false positive (classified as spam, but really not) is:')\n",
    "print('------------------------------------------------------------------------------------------')\n",
    "print(emails[0])\n",
    "print('------------------------------------------------------------------------------------------')\n",
    "print('The 1st email that is a false negative (classified as ham, but really spam) is:')\n",
    "print('------------------------------------------------------------------------------------------')\n",
    "print(emails[1])\n",
    "print('------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4) Comparison with KNN\n",
    "\n",
    "\n",
    "- Run a similar analysis to what you did with Naive Bayes above. When computing accuracy on the test set, you may want to reduce the size of the test set (e.g. to the first 500 emails in the test set).\n",
    "- Copy-paste your `confusion_matrix` method into `knn.py` so that you can run the same analysis on a KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knn import KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank you for your patience... this might take a while...\n"
     ]
    }
   ],
   "source": [
    "# Construct and train your KNN classifier\n",
    "knn = KNN(2)\n",
    "knn.train(x_train, y_train)\n",
    "print('thank you for your patience... this might take a while...')\n",
    "knn_ypred = knn.predict(x_test[:500],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn accuracy is :  0.912\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of the KNN classifier\n",
    "knn_acc = knn.accuracy(y_test[:500], knn_ypred[:500])\n",
    "print(\"knn accuracy is : \", knn_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[208.  41.]\n",
      " [  8. 243.]]\n"
     ]
    }
   ],
   "source": [
    "num_test = 500\n",
    "print(knn.confusion_matrix(y=y_test[:num_test], y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12:** What accuracy did you get on the test set (potentially reduced in size)?\n",
    "\n",
    "**Answer 12:** The accuracy is `91.2%`.\n",
    "\n",
    "**Question 13:** How does the confusion matrix compare to that obtained by Naive Bayes?\n",
    "\n",
    "**Answer 13:** The confusion matrix shows that there's fewer misclassification errors \n",
    "\n",
    "**Question 14:** Briefly describe at least one pro/con of KNN compared to Naive Bayes on this dataset.\n",
    "\n",
    "**Answer 14:** KNN gives us higher accuracy but it's more computational intense and memory intense.\n",
    "\n",
    "**Question 15:** When potentially reducing the size of the test set here, why is it important that we shuffled our train and test set?\n",
    "\n",
    "**Answer 15:** because it we don't shuffle the data, we might end up with a substantial amount of spam data but almost zero ham data or other way around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions (Section B only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Better text preprocessing\n",
    "\n",
    "- If you look at the top words extracted from the email dataset, many of them are common \"stop words\" (e.g. a, the, to, etc.) that do not carry much meaning when it comes to differentiating between spam vs. non-spam email. Improve your preprocessing pipeline by building your top words without stop words. Analyze performance differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources: https://www.ranks.nl/stopwords**\n",
    "\n",
    "I created a list of common stopwords in english and implemented the improved text preprocessing by adding a conditional statement in `count_word()`. If the word exists in the stopwords list, then we ignore it.\n",
    "\n",
    "After making the feature vector, train the classfier and predict the classes. We see that the accuracy is now `93%` which is higher than before `89%`.\n",
    "\n",
    "So the improved method of text preprocessing indeed boosted the performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without stopwords\n",
    "word_freq, num_emails = epp.count_words()\n",
    "top_words, counts = epp.find_top_words(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'and', 'of', 'a', 'in', 'for', 'you', 'is', 'this'] [290748, 213001, 157468, 148447, 118163, 106594, 84419, 82069, 71536, 63847]\n"
     ]
    }
   ],
   "source": [
    "print(top_words[:10], counts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with stopwords\n",
    "word_freq, num_emails = epp.count_words(stopwords = True)\n",
    "top_words, counts = epp.find_top_words(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enron', 'subject', 'ect', 'com', 'company', 'please', 'hou', 'e', 'would', 'new'] [60909, 47811, 35346, 24185, 22959, 20350, 17264, 16018, 15529, 15272]\n"
     ]
    }
   ],
   "source": [
    "print(top_words[:10], counts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make feature vector and divide train and test sets\n",
    "# np.random.seed(0)\n",
    "better_feats, better_y = epp.make_feature_vectors(top_words, num_emails)\n",
    "better_x_train, better_y_train, better_train_inds, better_x_test, better_y_test, better_test_inds = epp.make_train_test_sets(better_feats, better_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after text preprocessing with stopwords :  0.9267062314540059\n"
     ]
    }
   ],
   "source": [
    "nb.train(better_x_train, better_y_train)\n",
    "better_ypred = nb.predict(better_x_test)\n",
    "# print(y_test[:10])\n",
    "# print(better_ypred[:10])\n",
    "print(\"accuracy after text preprocessing with stopwords : \", nb.accuracy(better_y_test, better_ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature size\n",
    "\n",
    "- Explore how the number of selected features for the email dataset influences accuracy and runtime performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate the effect of number of features on the performance, I decided to run the Naive Bayes `find_top_words` with different number of features and see how the accuracy changes when predicting the same set of data.\n",
    "\n",
    "This extension is very computationally intense and time-consuming. After waiting for a loooooong time, the results are presented below.\n",
    "\n",
    "An intuitive observation is that the accuracy significantly increases as we have more features, but the downside is that the program takes longer time.\n",
    "\n",
    "Thus it having a threshold of accuracy would give us a balance between computation time and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...num feature 50\n",
      "running...num feature 100\n",
      "running...num feature 200\n",
      "running...num feature 400\n",
      "running...num feature 1000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# num features selected\n",
    "np.random.seed(0)\n",
    "num_features = [50, 100, 200, 400, 1000]\n",
    "runtime = []\n",
    "accuracy = []\n",
    "# run iteration\n",
    "print('thank you for the patience, this might take a while...')\n",
    "for num_f in num_features:\n",
    "    # indicator\n",
    "    print(f'running...num feature {num_f}')\n",
    "    s = time.time()\n",
    "    word_freq, num_emails = epp.count_words(stopwords = False)\n",
    "    top_words, counts = epp.find_top_words(word_freq, num_features = num_f)\n",
    "    better_feats, better_y = epp.make_feature_vectors(top_words, num_emails)\n",
    "    better_x_train, better_y_train, better_train_inds, better_x_test, better_y_test, better_test_inds = epp.make_train_test_sets(better_feats, better_y)\n",
    "    nb.train(better_x_train, better_y_train)\n",
    "    better_ypred = nb.predict(better_x_test)\n",
    "    acc = nb.accuracy(better_y_test, better_ypred)\n",
    "    e = time.time()\n",
    "    runtime.append(e-s)\n",
    "    accuracy.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "num of features:  [50, 100, 200, 400, 1000]\n",
      "runtime(s):  [15.48, 18.48, 24.03, 33.37, 56.82]\n",
      "accuracy:  [0.77, 0.85, 0.89, 0.94, 0.96]\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "print('-------------------------------')\n",
    "print('num of features: ', num_features)\n",
    "print('runtime(s): ', list(np.round(np.array(runtime),2)))\n",
    "print('accuracy: ', list(np.round(np.array(accuracy),2)))\n",
    "print('-------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Distance metrics\n",
    "- Compare KNN performance with the $L^2$ and $L^1$ distance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are shown below. Obviously, `l2` distance gives us higher accuracy than `l1` distance metrics. This is because we are voting based on more neighbors of the data sample. Correspondingly `l2` distance metrics will result in longer computational time.\n",
    "\n",
    "Surprisingly, after experimenting with higher degree distance metrics, the accuracy instead goes down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 distance | accuracy : 0.52\n",
      "l2 distance | accuracy : 0.93\n",
      "l3 distance | accuracy : 0.52\n"
     ]
    }
   ],
   "source": [
    "knn = KNN(2)\n",
    "# l1 dist\n",
    "knn.train(x_train, y_train)\n",
    "knn_ypred = knn.predict(x_test[:200], k = 2, dist_metrics = 'l1')\n",
    "print(f'l1 distance | accuracy : {knn.accuracy(y_test[:200], knn_ypred)}')\n",
    "# l2 dist\n",
    "knn_ypred = knn.predict(x_test[:200], k = 2, dist_metrics = 'l2')\n",
    "print(f'l2 distance | accuracy : {knn.accuracy(y_test[:200], knn_ypred)}')\n",
    "# l3 dist\n",
    "knn_ypred = knn.predict(x_test[:200], k = 2, dist_metrics = 'l3')\n",
    "print(f'l3 distance | accuracy : {knn.accuracy(y_test[:200], knn_ypred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
